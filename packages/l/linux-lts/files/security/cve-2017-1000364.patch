From: Michal Hocko <mhocko@suse.com>
Date: Wed, 14 Jun 2017 08:16:54 +0200
Subject: mm: enlarge stack guard gap

Stack guard page is a useful feature to reduce a risk of stack smashing
into a different mapping. We have been using a single page gap which
is sufficient to prevent having stack adjacent to a different mapping.
But this seems to be insufficient in the light of the stack usage in
the userspace. E.g. glibc uses as large as 64kB alloca() in many
commonly used functions. Others use constructs liks gid_t
buffer[NGROUPS_MAX] which is 256kB or stack strings with MAX_ARG_STRLEN.

This will become especially dangerous for suid binaries and the default
no limit for the stack size limit because those applications can be
tricked to consume a large portion of the stack and a single glibc call
could jump over the guard page. These attacks are not theoretical,
unfortunatelly.

Make those attacks less probable by increasing the stack guard gap
to 1MB (on systems with 4k pages but make it depend on the page size
because systems with larger base pages might cap stack allocations in
the PAGE_SIZE units) which should cover larger alloca() and VLA stack
allocations. It is obviously not a full fix because the problem is
somehow inherent but it should reduce attack space a lot. One could
argue that the gap size should be configurable from the userspace but
that can be done later on top when somebody finds that the new 1MB is
not suitable or even wrong for some special case applications.

Implementation wise, get rid of check_stack_guard_page and move all the
guard page specific code to expandable_stack_area which always tries to
guarantee the gap. do_anonymous_page then just calls expand_stack. Also
get rid of stack_guard_page_{start,end} and replace them with
stack_guard_area to handle stack population and /proc/<pid>/[s]maps.

This should clean up the code which is quite scattered currently
and therefore justify the change.

Signed-off-by: Michal Hocko <mhocko@suse.com>
[carnil: backport for 4.9: vmf->address -> fe->address]
---
 arch/ia64/mm/fault.c |   2 +-
 fs/exec.c            |   8 ++-
 fs/proc/task_mmu.c   |  11 ++--
 include/linux/mm.h   |  40 +++-----------
 mm/gup.c             |   4 +-
 mm/memory.c          |  38 ++-----------
 mm/mmap.c            | 152 +++++++++++++++++++++++++++++++++++++++++----------
 7 files changed, 152 insertions(+), 103 deletions(-)

--- a/arch/ia64/mm/fault.c
+++ b/arch/ia64/mm/fault.c
@@ -224,7 +224,7 @@ retry:
 		 */
 		if (address > vma->vm_end + PAGE_SIZE - sizeof(long))
 			goto bad_area;
-		if (expand_upwards(vma, address))
+		if (expand_upwards(vma, address, 0))
 			goto bad_area;
 	}
 	goto good_area;
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -196,7 +196,7 @@ static struct page *get_arg_page(struct
 
 #ifdef CONFIG_STACK_GROWSUP
 	if (write) {
-		ret = expand_downwards(bprm->vma, pos);
+		ret = expand_downwards(bprm->vma, pos, 0);
 		if (ret < 0)
 			return NULL;
 	}
@@ -218,6 +218,12 @@ static struct page *get_arg_page(struct
 		unsigned long size = bprm->vma->vm_end - bprm->vma->vm_start;
 		struct rlimit *rlim;
 
+		/*
+		 * GRWOSUP doesn't really have any gap at this stage because we grow
+		 * the stack down now. See the expand_downwards above.
+		 */
+		if (!IS_ENABLED(CONFIG_STACK_GROWSUP))
+			size -= stack_guard_gap;
 		acct_arg_size(bprm, size / PAGE_SIZE);
 
 		/*
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -302,11 +302,14 @@ show_map_vma(struct seq_file *m, struct
 
 	/* We don't show the stack guard page in /proc/maps */
 	start = vma->vm_start;
-	if (stack_guard_page_start(vma, start))
-		start += PAGE_SIZE;
 	end = vma->vm_end;
-	if (stack_guard_page_end(vma, end))
-		end -= PAGE_SIZE;
+	if (vma->vm_flags & VM_GROWSDOWN) {
+		if (stack_guard_area(vma, start))
+			start += stack_guard_gap;
+	} else if (vma->vm_flags & VM_GROWSUP) {
+		if (stack_guard_area(vma, end))
+			end -= stack_guard_gap;
+	}
 
 	seq_setwidth(m, 25 + sizeof(void *) * 6 - 1);
 	seq_printf(m, "%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu ",
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1378,39 +1378,11 @@ int clear_page_dirty_for_io(struct page
 
 int get_cmdline(struct task_struct *task, char *buffer, int buflen);
 
-/* Is the vma a continuation of the stack vma above it? */
-static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)
-{
-	return vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);
-}
-
 static inline bool vma_is_anonymous(struct vm_area_struct *vma)
 {
 	return !vma->vm_ops;
 }
 
-static inline int stack_guard_page_start(struct vm_area_struct *vma,
-					     unsigned long addr)
-{
-	return (vma->vm_flags & VM_GROWSDOWN) &&
-		(vma->vm_start == addr) &&
-		!vma_growsdown(vma->vm_prev, addr);
-}
-
-/* Is the vma a continuation of the stack vma below it? */
-static inline int vma_growsup(struct vm_area_struct *vma, unsigned long addr)
-{
-	return vma && (vma->vm_start == addr) && (vma->vm_flags & VM_GROWSUP);
-}
-
-static inline int stack_guard_page_end(struct vm_area_struct *vma,
-					   unsigned long addr)
-{
-	return (vma->vm_flags & VM_GROWSUP) &&
-		(vma->vm_end == addr) &&
-		!vma_growsup(vma->vm_next, addr);
-}
-
 int vma_is_stack_for_current(struct vm_area_struct *vma);
 
 extern unsigned long move_page_tables(struct vm_area_struct *vma,
@@ -2149,16 +2121,22 @@ void page_cache_async_readahead(struct a
 				pgoff_t offset,
 				unsigned long size);
 
+extern unsigned long stack_guard_gap;
 /* Generic expand stack which grows the stack according to GROWS{UP,DOWN} */
 extern int expand_stack(struct vm_area_struct *vma, unsigned long address);
+extern int stack_guard_area(struct vm_area_struct *vma, unsigned long address);
 
 /* CONFIG_STACK_GROWSUP still needs to to grow downwards at some places */
 extern int expand_downwards(struct vm_area_struct *vma,
-		unsigned long address);
+		unsigned long address, unsigned long gap);
+unsigned long expandable_stack_area(struct vm_area_struct *vma,
+		unsigned long address, unsigned long *gap);
+
 #if VM_GROWSUP
-extern int expand_upwards(struct vm_area_struct *vma, unsigned long address);
+extern int expand_upwards(struct vm_area_struct *vma,
+		unsigned long address, unsigned long gap);
 #else
-  #define expand_upwards(vma, address) (0)
+  #define expand_upwards(vma, address, gap) (0)
 #endif
 
 /* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -371,9 +371,7 @@ static int faultin_page(struct task_stru
 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
 		return -ENOENT;
 	/* For mm_populate(), just skip the stack guard page. */
-	if ((*flags & FOLL_POPULATE) &&
-			(stack_guard_page_start(vma, address) ||
-			 stack_guard_page_end(vma, address + PAGE_SIZE)))
+	if ((*flags & FOLL_POPULATE) && stack_guard_area(vma, address))
 		return -ENOENT;
 	if (*flags & FOLL_WRITE)
 		fault_flags |= FAULT_FLAG_WRITE;
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2698,39 +2698,7 @@ out_release:
 	return ret;
 }
 
-/*
- * This is like a special single-page "expand_{down|up}wards()",
- * except we must first make sure that 'address{-|+}PAGE_SIZE'
- * doesn't hit another vma.
- */
-static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)
-{
-	address &= PAGE_MASK;
-	if ((vma->vm_flags & VM_GROWSDOWN) && address == vma->vm_start) {
-		struct vm_area_struct *prev = vma->vm_prev;
-
-		/*
-		 * Is there a mapping abutting this one below?
-		 *
-		 * That's only ok if it's the same stack mapping
-		 * that has gotten split..
-		 */
-		if (prev && prev->vm_end == address)
-			return prev->vm_flags & VM_GROWSDOWN ? 0 : -ENOMEM;
-
-		return expand_downwards(vma, address - PAGE_SIZE);
-	}
-	if ((vma->vm_flags & VM_GROWSUP) && address + PAGE_SIZE == vma->vm_end) {
-		struct vm_area_struct *next = vma->vm_next;
 
-		/* As VM_GROWSDOWN but s/below/above/ */
-		if (next && next->vm_start == address + PAGE_SIZE)
-			return next->vm_flags & VM_GROWSUP ? 0 : -ENOMEM;
-
-		return expand_upwards(vma, address + PAGE_SIZE);
-	}
-	return 0;
-}
 
 /*
  * We enter with non-exclusive mmap_sem (to exclude vma changes,
@@ -2749,8 +2717,10 @@ static int do_anonymous_page(struct faul
 		return VM_FAULT_SIGBUS;
 
 	/* Check if we need to add a guard page to the stack */
-	if (check_stack_guard_page(vma, fe->address) < 0)
-		return VM_FAULT_SIGSEGV;
+	if (stack_guard_area(vma, fe->address)) {
+		if (expand_stack(vma, fe->address) < 0)
+			return VM_FAULT_SIGSEGV;
+	}
 
 	/*
 	 * Use pte_alloc() instead of pte_alloc_map().  We can't run
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -2151,7 +2151,8 @@ find_vma_prev(struct mm_struct *mm, unsi
  * update accounting. This is shared with both the
  * grow-up and grow-down cases.
  */
-static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow)
+static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, unsigned long grow,
+		unsigned long gap)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	struct rlimit *rlim = current->signal->rlim;
@@ -2164,7 +2165,7 @@ static int acct_stack_growth(struct vm_a
 	/* Stack limit test */
 	actual_size = size;
 	if (size && (vma->vm_flags & (VM_GROWSUP | VM_GROWSDOWN)))
-		actual_size -= PAGE_SIZE;
+		actual_size -= gap;
 	if (actual_size > READ_ONCE(rlim[RLIMIT_STACK].rlim_cur))
 		return -ENOMEM;
 
@@ -2200,7 +2201,7 @@ static int acct_stack_growth(struct vm_a
  * PA-RISC uses this for its stack; IA64 for its Register Backing Store.
  * vma is the last one with address > vma->vm_end.  Have to extend vma.
  */
-int expand_upwards(struct vm_area_struct *vma, unsigned long address)
+int expand_upwards(struct vm_area_struct *vma, unsigned long address, unsigned long gap)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	int error = 0;
@@ -2208,12 +2209,6 @@ int expand_upwards(struct vm_area_struct
 	if (!(vma->vm_flags & VM_GROWSUP))
 		return -EFAULT;
 
-	/* Guard against wrapping around to address 0. */
-	if (address < PAGE_ALIGN(address+4))
-		address = PAGE_ALIGN(address+4);
-	else
-		return -ENOMEM;
-
 	/* We must make sure the anon_vma is allocated. */
 	if (unlikely(anon_vma_prepare(vma)))
 		return -ENOMEM;
@@ -2234,7 +2229,7 @@ int expand_upwards(struct vm_area_struct
 
 		error = -ENOMEM;
 		if (vma->vm_pgoff + (size >> PAGE_SHIFT) >= vma->vm_pgoff) {
-			error = acct_stack_growth(vma, size, grow);
+			error = acct_stack_growth(vma, size, grow, gap);
 			if (!error) {
 				/*
 				 * vma_gap_update() doesn't support concurrent
@@ -2275,7 +2270,7 @@ int expand_upwards(struct vm_area_struct
  * vma is the first one with address < vma->vm_start.  Have to extend vma.
  */
 int expand_downwards(struct vm_area_struct *vma,
-				   unsigned long address)
+				   unsigned long address, unsigned long gap)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	int error;
@@ -2305,7 +2300,7 @@ int expand_downwards(struct vm_area_stru
 
 		error = -ENOMEM;
 		if (grow <= vma->vm_pgoff) {
-			error = acct_stack_growth(vma, size, grow);
+			error = acct_stack_growth(vma, size, grow, gap);
 			if (!error) {
 				/*
 				 * vma_gap_update() doesn't support concurrent
@@ -2339,29 +2334,72 @@ int expand_downwards(struct vm_area_stru
 	return error;
 }
 
+/* enforced gap between the expanding stack and other mappings. */
+unsigned long stack_guard_gap = 256UL<<PAGE_SHIFT;
+
 /*
  * Note how expand_stack() refuses to expand the stack all the way to
  * abut the next virtual mapping, *unless* that mapping itself is also
- * a stack mapping. We want to leave room for a guard page, after all
+ * a stack mapping. We want to leave room for a guard area, after all
  * (the guard page itself is not added here, that is done by the
  * actual page faulting logic)
- *
- * This matches the behavior of the guard page logic (see mm/memory.c:
- * check_stack_guard_page()), which only allows the guard page to be
- * removed under these circumstances.
  */
 #ifdef CONFIG_STACK_GROWSUP
+unsigned long expandable_stack_area(struct vm_area_struct *vma,
+		unsigned long address, unsigned long *gap)
+{
+	struct vm_area_struct *next = vma->vm_next;
+	unsigned long guard_gap = stack_guard_gap;
+	unsigned long guard_addr;
+
+	address = ALIGN(address, PAGE_SIZE);;
+	if (!next)
+		goto out;
+
+	if (next->vm_flags & VM_GROWSUP) {
+		guard_gap = min(guard_gap, next->vm_start - address);
+		goto out;
+	}
+
+	if (next->vm_start - address < guard_gap)
+		return -ENOMEM;
+out:
+	if (TASK_SIZE - address < guard_gap)
+		guard_gap = TASK_SIZE - address;
+	guard_addr = address + guard_gap;
+	*gap = guard_gap;
+
+	return guard_addr;
+}
+
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
+	unsigned long gap;
+
+	address = expandable_stack_area(vma, address, &gap);
+	if (IS_ERR_VALUE(address))
+		return -ENOMEM;
+	return expand_upwards(vma, address, gap);
+}
+
+int stack_guard_area(struct vm_area_struct *vma, unsigned long address)
+{
 	struct vm_area_struct *next;
 
-	address &= PAGE_MASK;
+	if (!(vma->vm_flags & VM_GROWSUP))
+		return 0;
+
+	/*
+	 * strictly speaking there is a guard gap between disjoint stacks
+	 * but the gap is not canonical (it might be smaller) and it is
+	 * reasonably safe to assume that we can ignore that gap for stack
+	 * POPULATE or /proc/<pid>[s]maps purposes
+	 */
 	next = vma->vm_next;
-	if (next && next->vm_start == address + PAGE_SIZE) {
-		if (!(next->vm_flags & VM_GROWSUP))
-			return -ENOMEM;
-	}
-	return expand_upwards(vma, address);
+	if (next && next->vm_flags & VM_GROWSUP)
+		return 0;
+
+	return vma->vm_end - address <= stack_guard_gap;
 }
 
 struct vm_area_struct *
@@ -2380,17 +2418,73 @@ find_extend_vma(struct mm_struct *mm, un
 	return prev;
 }
 #else
+unsigned long expandable_stack_area(struct vm_area_struct *vma,
+		unsigned long address, unsigned long *gap)
+{
+	struct vm_area_struct *prev = vma->vm_prev;
+	unsigned long guard_gap = stack_guard_gap;
+	unsigned long guard_addr;
+
+	address &= PAGE_MASK;
+	if (!prev)
+		goto out;
+
+	/*
+	 * Is there a mapping abutting this one below?
+	 *
+	 * That's only ok if it's the same stack mapping
+	 * that has gotten split or there is sufficient gap
+	 * between mappings
+	 */
+	if (prev->vm_flags & VM_GROWSDOWN) {
+		guard_gap = min(guard_gap, address - prev->vm_end);
+		goto out;
+	}
+
+	if (address - prev->vm_end < guard_gap)
+		return -ENOMEM;
+
+out:
+	/* make sure we won't underflow */
+	if (address < mmap_min_addr)
+		return -ENOMEM;
+	if (address - mmap_min_addr < guard_gap)
+		guard_gap = address - mmap_min_addr;
+
+	guard_addr = address - guard_gap;
+	*gap = guard_gap;
+
+	return guard_addr;
+}
+
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
+	unsigned long gap;
+
+	address = expandable_stack_area(vma, address, &gap);
+	if (IS_ERR_VALUE(address))
+		return -ENOMEM;
+	return expand_downwards(vma, address, gap);
+}
+
+int stack_guard_area(struct vm_area_struct *vma, unsigned long address)
+{
 	struct vm_area_struct *prev;
 
-	address &= PAGE_MASK;
+	if (!(vma->vm_flags & VM_GROWSDOWN))
+		return 0;
+
+	/*
+	 * strictly speaking there is a guard gap between disjoint stacks
+	 * but the gap is not canonical (it might be smaller) and it is
+	 * reasonably safe to assume that we can ignore that gap for stack
+	 * POPULATE or /proc/<pid>[s]maps purposes
+	 */
 	prev = vma->vm_prev;
-	if (prev && prev->vm_end == address) {
-		if (!(prev->vm_flags & VM_GROWSDOWN))
-			return -ENOMEM;
-	}
-	return expand_downwards(vma, address);
+	if (prev && prev->vm_flags & VM_GROWSDOWN)
+		return 0;
+
+	return address - vma->vm_start < stack_guard_gap;
 }
 
 struct vm_area_struct *
--

From: Michal Hocko <mhocko@suse.com>
Date: Wed, 14 Jun 2017 08:17:02 +0200
Subject: mm: allow to configure stack gap size

Add a kernel command line option (stack_guard_gap) to specify the stack
gap size (in page unites) and export the value in /proc/<pid>/smaps for
stack vmas. This might be used for special applications like CRIU/RR.

Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Michal Hocko <mhocko@suse.com>
[carnil: backport to 4.9, adjust location for documentation]
---
 Documentation/kernel-parameters.txt |  7 +++++++
 fs/proc/task_mmu.c                  |  3 +++
 mm/mmap.c                           | 13 +++++++++++++
 3 files changed, 23 insertions(+)

--- a/Documentation/kernel-parameters.txt
+++ b/Documentation/kernel-parameters.txt
@@ -3932,6 +3932,13 @@ bytes respectively. Such letter suffixes
 	spia_pedr=
 	spia_peddr=
 
+	stack_guard_gap=	[MM]
+			override the default stack gap protection. The value
+			is in page units and it defines how many pages prior
+			to (for stacks growing down) resp. after (for stacks
+			growing up) the main stack are reserved for no other
+			mapping. Default value is 256 pages.
+
 	stacktrace	[FTRACE]
 			Enabled the stack tracer on boot up.
 
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -805,6 +805,9 @@ static int show_smap(struct seq_file *m,
 		   (vma->vm_flags & VM_LOCKED) ?
 			(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);
 
+	if (is_stack(m->private, vma))
+		seq_printf(m, "Stack_Gap:      %8lu kB\n", stack_guard_gap >>10);
+
 	arch_show_smap(m, vma);
 	show_smap_vma_flags(m, vma);
 	m_cache_vma(m, vma);
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -2337,6 +2337,19 @@ int expand_downwards(struct vm_area_stru
 /* enforced gap between the expanding stack and other mappings. */
 unsigned long stack_guard_gap = 256UL<<PAGE_SHIFT;
 
+static int __init cmdline_parse_stack_guard_gap(char *p)
+{
+	unsigned long val;
+	char *endptr;
+
+	val = simple_strtoul(p, &endptr, 10);
+	if (!*endptr)
+		stack_guard_gap = val << PAGE_SHIFT;
+
+	return 0;
+}
+__setup("stack_guard_gap=", cmdline_parse_stack_guard_gap);
+
 /*
  * Note how expand_stack() refuses to expand the stack all the way to
  * abut the next virtual mapping, *unless* that mapping itself is also
--

From: Michal Hocko <mhocko@suse.com>
Date: Wed, 14 Jun 2017 08:17:15 +0200
Subject: mm, proc: cap the stack gap for unpopulated growing vmas

Oleg has noticed that show_map_vma has been overly eager to cut the
the vma range for growing VMAs. This wasn't a big deal with 4kB stack
gap but now that the gap is much larger we can simply get a bogus VMA
range in show_map_vma.
To quote Oleg
: On ppc PAGE_SIZE == 64K, so stack_guard_gap == 16M, the application does
: mmap(..., length=4M, ... MAP_GROWSDOWN) and /proc/pid/maps happily reports
:
:       30001000000-30000400000 rw-p 00000000 00:00 0

Let's cap the reported range and show an empty range for this peculiar
case which is what we have been doing for a long time.  Note that the
range will expand as soon as the first page fault happens on this range.

Reported-by: Jan Stancek <jstancek@redhat.com>
Signed-off-by: Michal Hocko <mhocko@suse.com>
---
 fs/proc/task_mmu.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 5ef3e14d235f..6f38f96d725f 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -303,10 +303,10 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)
 	end = vma->vm_end;
 	if (vma->vm_flags & VM_GROWSDOWN) {
 		if (stack_guard_area(vma, start))
-			start += stack_guard_gap;
+			start = min(end, start + stack_guard_gap);
 	} else if (vma->vm_flags & VM_GROWSUP) {
 		if (stack_guard_area(vma, end))
-			end -= stack_guard_gap;
+			end = max(start, end - stack_guard_gap);
 	}
 
 	seq_setwidth(m, 25 + sizeof(void *) * 6 - 1);
-- 
2.11.0

From: Michal Hocko <mhocko@suse.com>
Date: Wed, 14 Jun 2017 08:17:20 +0200
Subject: mm, proc: drop priv parameter from is_stack

We do not really need priv parameter since b18cb64ead40 ("fs/proc: Stop
trying to report thread stacks") simplified is_stack.

Signed-off-by: Michal Hocko <mhocko@suse.com>
---
 fs/proc/task_mmu.c   | 10 ++++------
 fs/proc/task_nommu.c |  5 ++---
 2 files changed, 6 insertions(+), 9 deletions(-)

diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index 6f38f96d725f..f05faa18d8b6 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -266,8 +266,7 @@ static int do_maps_open(struct inode *inode, struct file *file,
  * Indicate if the VMA is a stack for the given task; for
  * /proc/PID/maps that is the stack of the main task.
  */
-static int is_stack(struct proc_maps_private *priv,
-		    struct vm_area_struct *vma)
+static int is_stack(struct vm_area_struct *vma)
 {
 	/*
 	 * We make no effort to guess what a given thread considers to be
@@ -283,7 +282,6 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	struct file *file = vma->vm_file;
-	struct proc_maps_private *priv = m->private;
 	vm_flags_t flags = vma->vm_flags;
 	unsigned long ino = 0;
 	unsigned long long pgoff = 0;
@@ -349,7 +347,7 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)
 			goto done;
 		}
 
-		if (is_stack(priv, vma))
+		if (is_stack(vma))
 			name = "[stack]";
 	}
 
@@ -809,7 +807,7 @@ static int show_smap(struct seq_file *m, void *v, int is_pid)
 		   (vma->vm_flags & VM_LOCKED) ?
 			(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);
 
-	if (is_stack(m->private, vma))
+	if (is_stack(vma))
 		seq_printf(m, "Stack_Gap:      %8lu kB\n", stack_guard_gap >>10);
 
 	arch_show_smap(m, vma);
@@ -1681,7 +1679,7 @@ static int show_numa_map(struct seq_file *m, void *v, int is_pid)
 		seq_file_path(m, file, "\n\t= ");
 	} else if (vma->vm_start <= mm->brk && vma->vm_end >= mm->start_brk) {
 		seq_puts(m, " heap");
-	} else if (is_stack(proc_priv, vma)) {
+	} else if (is_stack(vma)) {
 		seq_puts(m, " stack");
 	}
 
diff --git a/fs/proc/task_nommu.c b/fs/proc/task_nommu.c
index 23266694db11..dea90b566a6e 100644
--- a/fs/proc/task_nommu.c
+++ b/fs/proc/task_nommu.c
@@ -125,8 +125,7 @@ unsigned long task_statm(struct mm_struct *mm,
 	return size;
 }
 
-static int is_stack(struct proc_maps_private *priv,
-		    struct vm_area_struct *vma)
+static int is_stack(struct vm_area_struct *vma)
 {
 	struct mm_struct *mm = vma->vm_mm;
 
@@ -178,7 +177,7 @@ static int nommu_vma_show(struct seq_file *m, struct vm_area_struct *vma,
 	if (file) {
 		seq_pad(m, ' ');
 		seq_file_path(m, file, "");
-	} else if (mm && is_stack(priv, vma)) {
+	} else if (mm && is_stack(vma)) {
 		seq_pad(m, ' ');
 		seq_printf(m, "[stack]");
 	}
-- 
2.11.0

From: Michal Hocko <mhocko@suse.com>
Date: Wed, 14 Jun 2017 08:18:00 +0200
Subject: mm: do not collapse stack gap into THP

Oleg has noticed that khugepaged will happilly collapse stack vma (as
long as it is not an early stack - see is_vma_temporary_stack) and
it might effectively remove the stack gap area as well because a larger
part of the stack vma is usually populated. The same applies to the
page fault handler.

Fix this by checking stack_guard_area when revalidating a VMA
in hugepage_vma_revalidate.  We do not want to hook/replace
is_vma_temporary_stack() check because THP might be still useful for
stack, all we need is excluding the gap from collapsing into a THP.

Also check the to-be-created THP in do_huge_pmd_anonymous_page to
make sure it is completely outside of the gap area because we we could
create THP covering the gap area.

Noticed-by: Oleg Nesterov <oleg@redhat.com>
Signed-off-by: Michal Hocko <mhocko@suse.com>
---
 mm/huge_memory.c | 3 +++
 mm/khugepaged.c  | 4 ++++
 2 files changed, 7 insertions(+)

diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index a84909cf20d3..a8b6881bb2a2 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -664,6 +664,9 @@ int do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 
 	if (haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end)
 		return VM_FAULT_FALLBACK;
+	if (stack_guard_area(vma, haddr) ||
+			stack_guard_area(vma, haddr + HPAGE_PMD_SIZE))
+		return VM_FAULT_FALLBACK;
 	if (unlikely(anon_vma_prepare(vma)))
 		return VM_FAULT_OOM;
 	if (unlikely(khugepaged_enter(vma, vma->vm_flags)))
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index 945fd1ca49b5..75d7fff36957 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -858,6 +858,10 @@ static int hugepage_vma_revalidate(struct mm_struct *mm, unsigned long address,
 		return SCAN_ADDRESS_RANGE;
 	if (!hugepage_vma_check(vma))
 		return SCAN_VMA_CHECK;
+
+	/* never try to collapse stack gap */
+	if (stack_guard_area(vma, hstart) || stack_guard_area(vma, hend))
+		return SCAN_ADDRESS_RANGE;
 	return 0;
 }
 
-- 
2.11.0

From: Michal Hocko <mhocko@suse.com>
Date: Fri, 16 Jun 2017 00:06:28 +0200
Subject: fold me "mm: allow to configure stack gap size"

- do not rely on is_stack when reporting the gap. show_map_vma has
  all the information we need
---
 fs/proc/task_mmu.c | 19 +++++++++++++------
 1 file changed, 13 insertions(+), 6 deletions(-)

diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
index f05faa18d8b6..ec7abc90b844 100644
--- a/fs/proc/task_mmu.c
+++ b/fs/proc/task_mmu.c
@@ -278,7 +278,7 @@ static int is_stack(struct vm_area_struct *vma)
 }
 
 static void
-show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)
+show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid, bool *has_gap)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	struct file *file = vma->vm_file;
@@ -300,11 +300,17 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)
 	start = vma->vm_start;
 	end = vma->vm_end;
 	if (vma->vm_flags & VM_GROWSDOWN) {
-		if (stack_guard_area(vma, start))
+		if (stack_guard_area(vma, start)) {
 			start = min(end, start + stack_guard_gap);
+			if (has_gap)
+				*has_gap = true;
+		}
 	} else if (vma->vm_flags & VM_GROWSUP) {
-		if (stack_guard_area(vma, end))
+		if (stack_guard_area(vma, end)) {
 			end = max(start, end - stack_guard_gap);
+			if (has_gap)
+				*has_gap = true;
+		}
 	}
 
 	seq_setwidth(m, 25 + sizeof(void *) * 6 - 1);
@@ -361,7 +367,7 @@ show_map_vma(struct seq_file *m, struct vm_area_struct *vma, int is_pid)
 
 static int show_map(struct seq_file *m, void *v, int is_pid)
 {
-	show_map_vma(m, v, is_pid);
+	show_map_vma(m, v, is_pid, NULL);
 	m_cache_vma(m, v);
 	return 0;
 }
@@ -734,6 +740,7 @@ static int show_smap(struct seq_file *m, void *v, int is_pid)
 		.mm = vma->vm_mm,
 		.private = &mss,
 	};
+	bool has_gap = false;
 
 	memset(&mss, 0, sizeof mss);
 
@@ -764,7 +771,7 @@ static int show_smap(struct seq_file *m, void *v, int is_pid)
 	/* mmap_sem is held in m_start */
 	walk_page_vma(vma, &smaps_walk);
 
-	show_map_vma(m, vma, is_pid);
+	show_map_vma(m, vma, is_pid, &has_gap);
 
 	seq_printf(m,
 		   "Size:           %8lu kB\n"
@@ -807,7 +814,7 @@ static int show_smap(struct seq_file *m, void *v, int is_pid)
 		   (vma->vm_flags & VM_LOCKED) ?
 			(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);
 
-	if (is_stack(vma))
+	if (has_gap)
 		seq_printf(m, "Stack_Gap:      %8lu kB\n", stack_guard_gap >>10);
 
 	arch_show_smap(m, vma);
-- 
2.11.0

From f4cb767d76cf7ee72f97dd76f6cfa6c76a5edc89 Mon Sep 17 00:00:00 2001
From: Hugh Dickins <hughd@google.com>
Date: Tue, 20 Jun 2017 02:10:44 -0700
Subject: mm: fix new crash in unmapped_area_topdown()

Trinity gets kernel BUG at mm/mmap.c:1963! in about 3 minutes of
mmap testing.  That's the VM_BUG_ON(gap_end < gap_start) at the
end of unmapped_area_topdown().  Linus points out how MAP_FIXED
(which does not have to respect our stack guard gap intentions)
could result in gap_end below gap_start there.  Fix that, and
the similar case in its alternative, unmapped_area().

Cc: stable@vger.kernel.org
Fixes: 1be7107fbe18 ("mm: larger stack guard gap, between vmas")
Reported-by: Dave Jones <davej@codemonkey.org.uk>
Debugged-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Hugh Dickins <hughd@google.com>
Acked-by: Michal Hocko <mhocko@suse.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
---
 mm/mmap.c | 6 ++++--
 1 file changed, 4 insertions(+), 2 deletions(-)

diff --git a/mm/mmap.c b/mm/mmap.c
index 8e07976..290b77d 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1817,7 +1817,8 @@ check_current:
 		/* Check if current node has a suitable gap */
 		if (gap_start > high_limit)
 			return -ENOMEM;
-		if (gap_end >= low_limit && gap_end - gap_start >= length)
+		if (gap_end >= low_limit &&
+		    gap_end > gap_start && gap_end - gap_start >= length)
 			goto found;
 
 		/* Visit right subtree if it looks promising */
@@ -1920,7 +1921,8 @@ check_current:
 		gap_end = vm_start_gap(vma);
 		if (gap_end < low_limit)
 			return -ENOMEM;
-		if (gap_start <= high_limit && gap_end - gap_start >= length)
+		if (gap_start <= high_limit &&
+		    gap_end > gap_start && gap_end - gap_start >= length)
 			goto found;
 
 		/* Visit left subtree if it looks promising */
-- 
cgit v1.1
